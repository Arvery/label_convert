dataset:
  name: CUB_200_2011
  root: E:\dataset\classifition\CUB_200_2011\CUB_200_2011
  train:
  val:


  # transform: you can diy your dataaug,
  # "Resize": [600,600]
  # "RandomCrop": [448,448]
  # "CenterCrop": [448,448]
  # "AutoAugImageNetPolicy":
  # "RandomHorizontalFlip":
  # "ToTensor":
  # "Normalize": [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]
train_transforms: { "Resize": [ 600,600 ],"RandomCrop": [ 448,448 ],"RandomHorizontalFlip": ,"ToTensor":, "Normalize": [ [ 0.485, 0.456, 0.406 ],[ 0.229, 0.224, 0.225 ] ] }
val_transforms: { "Resize": [ 600,600 ], "CenterCrop": [ 448,448 ],"RandomHorizontalFlip": ,"ToTensor":, "Normalize": [ [ 0.485, 0.456, 0.406 ],[ 0.229, 0.224, 0.225 ] ] }

model:
  name: ViT-B_16            # choice:"ViT-B_16","ViT-B_32","ViT-L_16","ViT-L_32","ViT-H_14"
  image_size: [ 448,448 ]   # image_size must same with RandomCrop and CenterCrop

  patches:
    patch_size: 16
    slide_step: 12
    split_type: 'non-overlap'
    hidden_size: 768

  transformer:
    mlp_dim: 3072
    num_heads: 12
    num_layers: 12
    attention_dropout_rate: 0.0
    dropout_rate: 0.1
    action: 'gelu'        # choice: "gelu","relu","swish"

  classifier: 'token'
  representation:

train:
  # log
  logging_name: TransFG_CUB              # logging name

  # device
  local_rank: -1                  # local_rank for distributed training on gpus, default=-1
  device: cuda                    # device
  nprocs: 1                       # nprocs for distributed training

  # dataloader
  batch_size: 16                  # batch_size, train and eval is same
  num_workers: 4                  # num_worker, windows change num_workers=0
  data_len:                       # if empty,use all data

  # model
  num_classes: 200                # num_classes
  np_weights: pretrain_weights/imagenet21k_ViT-B_16.npz   # ImageNet pretrain model (npz)
  pretrain_weights:               # pretrain weights path
  fp16: False                      # Whether to use 16-bit float precision instead of 32-bit

  # train and eval
  gradient_accumulation_steps: 1  # Number of updates steps to accumulate before performing a backward/update pass
  max_grad_norm: 1.0              # Max gradient norm
  eval_every: 100                 # Run prediction on validation set every so many steps. Will always run one evaluation at the end of training.

  # loss
  smoothing_value: 0              # if smoothing_value == 0,loss=nn.CrossEntropyLoss, else,loss=LabelSmoothing

optimizer: # optimizer
  name: SGD
  params:
    lr: 0.03
    momentum: 0.9
    weight_decay: 0.0001

# lr_schedule
scheduler:
  name: 'WarmupCosine'  # choice: WarmupLinear /  WarmupCosine default: WarmupCosine
  params:
    warmup_steps: 500   # Step of training to perform learning rate warmup for
    t_total: 10000      # Total number of training epochs to perform

# loss  loss design in model
criterion:
#  name: 'CrossEntropy'    # if smoothing_value=0 criterion=CrossEntropy,else LabelSmoothing
#  params:
#    weight:





